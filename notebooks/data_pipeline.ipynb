{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import drms\n",
    "from drms import DrmsQueryError\n",
    "from astropy.time import Time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "from os.path import exists\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initisalise drms client and display available data series\n",
    "c = drms.Client()\n",
    "c.series(r'hmi\\.sharp_')\n",
    "\n",
    "# Set a series\n",
    "si = c.info('hmi.sharp_720s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise SHARP metadata features.\n",
    "fields = [\n",
    "    \"T_REC\", \n",
    "    \"HARPNUM\", \n",
    "    \"NOAA_NUM\", \n",
    "    \"NOAA_ARS\", \n",
    "    \"NOAA_AR\", \n",
    "    \"QUALITY\", \n",
    "    \"TOTUSJH\", \n",
    "    \"TOTUSJZ\", \n",
    "    \"SAVNCPP\", \n",
    "    \"USFLUX\", \n",
    "    \"ABSNJZH\", \n",
    "    \"TOTPOT\",\n",
    "    \"SIZE_ACR\", \n",
    "    \"NACR\", \n",
    "    \"MEANPOT\", \n",
    "    \"SIZE\", \n",
    "    \"MEANJZH\", \n",
    "    \"SHRGT45\", \n",
    "    \"MEANSHR\",\n",
    "    \"MEANJZD\", \n",
    "    \"MEANALP\", \n",
    "    \"MEANGBT\", \n",
    "    \"MEANGBL\", \n",
    "    \"MEANGAM\", \n",
    "    \"MEANGBZ\", \n",
    "    \"MEANGBH\", \n",
    "    \"NPIX\"\n",
    "]\n",
    "\n",
    "query_string = \",\".join(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define log and error files for download\n",
    "\n",
    "log_file = \"D:/GitHub/solar-forecasting/logs/query_pipeline_log.csv\"\n",
    "error_file = \"D:/GitHub/solar-forecasting/logs/query_pipeline_error.csv\"\n",
    "\n",
    "def log_success(log_file, t1_str, t2_str, n_rows):\n",
    "    write_header = not exists(log_file)\n",
    "    with open(log_file, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if write_header:\n",
    "            writer.writerow(['start_time', 'end_time', 'rows_written'])\n",
    "        writer.writerow([t1_str, t2_str, n_rows])\n",
    "\n",
    "def log_error(error_file, t1_str, t2_str, error):\n",
    "    write_header = not exists(error_file)\n",
    "    with open(error_file, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if write_header:\n",
    "            writer.writerow(['start_time', 'end_time', 'error_message'])\n",
    "        writer.writerow([t1_str, t2_str, str(error)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download data to csv, searching iteratively for the most recent available day's data.\n",
    "\n",
    "time_diff = timedelta(minutes=1440)\n",
    "\n",
    "first_write = True\n",
    "end = Time.now()\n",
    "start = end - time_diff\n",
    "while True:\n",
    "    \n",
    "    print(f\"start = {start}\")\n",
    "    print(f\"end = {end}\")\n",
    "\n",
    "    print(f\"Downloading data\")\n",
    "    t_2_str = end.strftime(\"%Y.%m.%d_%H:%M:%S_TAI\") \n",
    "    t_1_str = start.strftime(\"%Y.%m.%d_%H:%M:%S_TAI\") \n",
    "    #t_1_str = \"2025.07.03_11:12:00_TAI\"\n",
    "\n",
    "    print(t_2_str)\n",
    "    print(t_1_str)\n",
    "    try:\n",
    "        extract = c.query(f'hmi.sharp_720s[1-13459][{t_1_str}-{t_2_str}]', key=query_string)\n",
    "\n",
    "        if not extract.empty:\n",
    "            extract.to_csv(\"D:/GitHub/solar-forecasting/data/sharp_metadata_pipeline_2.csv\", mode='a', index=False, header=first_write)\n",
    "            print(f\"Wrote {len(extract)} rows for {t_1_str} - {t_2_str}\")\n",
    "            log_success(log_file, start, end, len(extract))\n",
    "            if first_write:\n",
    "                first_write = False\n",
    "                start = start - time_diff\n",
    "                end = end - time_diff\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            print(f\"No records available for {t_1_str} - {t_2_str}\")\n",
    "            log_success(log_file, start, end, 0)\n",
    "            print(\"retry\")\n",
    "            start = start - time_diff\n",
    "            end = end - time_diff\n",
    "\n",
    "    except (DrmsQueryError, TimeoutError) as e:\n",
    "        print(f\"JSOC query failed for {t_1_str}-{t_2_str}: {e}\")\n",
    "        log_error(error_file, start, end, e)\n",
    "        print(\"retry\")\n",
    "        start = start - time_diff\n",
    "        end = end - time_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"D:/GitHub/solar-forecasting/data/sharp_metadata_pipeline_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Majority of NOAA_ARS are MISSING, drop column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa_ars_counts = data['NOAA_ARS'].value_counts()\n",
    "\n",
    "print(noaa_ars_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns='NOAA_ARS', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Replace all 'MISSING' strings with np.nan\n",
    "data.replace(['MISSING', 'NaN'], np.nan, inplace=True)\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retain only QUALITY data. Other data may be corrupted and unsuitable for training.\n",
    "data['QUALITY'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_data = data#[(data['QUALITY'] == 0)]# | (data['QUALITY'] == 65536)]\n",
    "quality_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_data['T_REC'] = pd.to_datetime(quality_data['T_REC'].str.replace('_TAI', ''), format='%Y.%m.%d_%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set pandas display options to show all rows (for large columns)\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns (if needed)\n",
    "\n",
    "# Now print the column (replace 'T_REC' with the column you want to print)\n",
    "print(quality_data['T_REC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Null values that do not suit linear interpolation, repair using median value.\n",
    "median_features = ['SIZE_ACR', 'SIZE', 'NPIX', 'NACR']\n",
    "\n",
    "for feature in median_features:\n",
    "    medians = quality_data.groupby('HARPNUM')[feature].transform('median')\n",
    "    quality_data[feature] = quality_data[feature].fillna(medians)\n",
    "\n",
    "quality_data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For other features, repair using linear interpolation, which is logical for time dependent magnetic flux measurements.\n",
    "linear_interpolation_features = ['TOTUSJH','TOTUSJZ', 'SAVNCPP', 'USFLUX', 'ABSNJZH', 'TOTPOT', 'MEANPOT', 'MEANJZH', 'SHRGT45', 'MEANSHR', 'MEANJZD', 'MEANALP', 'MEANGBT', 'MEANGBL', 'MEANGAM', 'MEANGBZ', 'MEANGBH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_data_LI = quality_data.copy()\n",
    "quality_data_LI[linear_interpolation_features] = quality_data[linear_interpolation_features].apply(\n",
    "    pd.to_numeric, errors='coerce'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = quality_data_LI.sort_values(['HARPNUM', 'T_REC']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in linear_interpolation_features:\n",
    "    df_sorted[col] = df_sorted.groupby('HARPNUM')[col].transform(lambda g: g.interpolate(method='linear', limit_direction = 'both'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop small number of remaining NULLs which could not be repaired using linear interpolation.\n",
    "#quality_data_no_null = df_sorted.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quality_data_no_null.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(quality_data['HARPNUM'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segment data into a dictionary containing HARPNUM (patch ID) as key, and list of SHARP sequences as values.\n",
    "harp_dict = {}\n",
    "\n",
    "grouped = quality_data.groupby('HARPNUM')\n",
    "\n",
    "for harpnum, group in grouped:\n",
    "    harp_dict[harpnum] = group\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(harp_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 30  # 6 hours of 12-minute cadence\n",
    "cadence_upper = pd.Timedelta(minutes=13)\n",
    "cadence_lower = pd.Timedelta(minutes=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_dict = {}\n",
    "\n",
    "for harp_ID, sample in harp_dict.items():\n",
    "      \n",
    "    valid_sequences = []\n",
    "    sample = sample.sort_values('T_REC').reset_index(drop=True)\n",
    "\n",
    "    start_idx = 0\n",
    "    while start_idx < (len(sample) - sequence_length + 1):\n",
    "            seq = sample.iloc[start_idx : start_idx + sequence_length]\n",
    "            time_deltas = seq['T_REC'].diff().dropna()\n",
    "\n",
    "            if all(time_deltas < cadence_upper) and all(time_deltas > cadence_lower):\n",
    "                valid_sequences.append(seq.reset_index(drop=True))\n",
    "                start_idx = start_idx + sequence_length\n",
    "            else:\n",
    "                 start_idx += 1\n",
    "    if len(valid_sequences) > 0:\n",
    "        sequence_dict[harp_ID] = valid_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sequence_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequence_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', 1000)   \n",
    "a = sequence_dict[13432][1].head()\n",
    "a.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to collect new rows\n",
    "new_rows = []\n",
    "\n",
    "# Assuming `sequence_dict` is already defined\n",
    "for harpnum, sequences in sequence_dict.items():\n",
    "    for index, sequence in enumerate(sequences):\n",
    "        # Add the row to the list (instead of concatenating repeatedly)\n",
    "        new_rows.append({\n",
    "            \"Harpnum\": harpnum,\n",
    "            \"T_REC\": sequence[\"T_REC\"].iloc[29],\n",
    "            \"seq_number\": index\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "sequence_end_time = pd.DataFrame(new_rows, columns=[\"Harpnum\", \"T_REC\", \"seq_number\"])\n",
    "\n",
    "# Ensure 'T_REC' is in datetime format\n",
    "sequence_end_time['T_REC'] = pd.to_datetime(sequence_end_time['T_REC'])\n",
    "\n",
    "# Find the maximum T_REC (end time)\n",
    "max_end_time = sequence_end_time['T_REC'].max()\n",
    "\n",
    "# Filter the DataFrame to keep only the rows where T_REC is equal to the max end time\n",
    "max_end_time_records = sequence_end_time[sequence_end_time['T_REC'] == max_end_time]\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "#print(max_end_time_records)\n",
    "\n",
    "recent_sequence_dict = {}\n",
    "\n",
    "# Assuming `max_end_time_records` is the DataFrame and `harpnum` is the key you're filtering by\n",
    "for harpnum, sequences in sequence_dict.items():\n",
    "    # Check if the harpnum exists in max_end_time_records to avoid errors\n",
    "    if harpnum in max_end_time_records['Harpnum'].values:\n",
    "        seq_index = max_end_time_records[max_end_time_records['Harpnum'] == harpnum]['seq_number'].iloc[0]\n",
    "        recent_sequence_dict[harpnum] = sequences[seq_index]\n",
    "    else:\n",
    "        print(f\"Warning: {harpnum} not found in max_end_time_records.\")\n",
    "\n",
    "# Print the resulting dictionary with the most recent sequences\n",
    "#print(recent_sequence_dict[13424])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list = []\n",
    "for harpnum, sequence in recent_sequence_dict.items():\n",
    "\n",
    "    #sequence_array = sequence.select_dtypes(include='number').to_numpy()\n",
    "    sequence_array = sequence.drop(columns=['T_REC', 'HARPNUM', 'NOAA_NUM', 'NOAA_AR', 'QUALITY']).to_numpy()\n",
    "    if sequence_array.shape[0] != 30:\n",
    "        print(f\"Sequence != 30 {harpnum}\")\n",
    "        continue  # optional: skip sequences that don't match expected length\n",
    "\n",
    "    X_list.append(sequence_array)\n",
    "\n",
    "print(len(X_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recent_sequence_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Initialize a list to store scaled sequences\n",
    "X_real_time_scaled = []\n",
    "\n",
    "# Load the pre-fitted scaler (from training data)\n",
    "scaler = joblib.load('D:/GitHub/solar-forecasting/data/scaler.pkl')\n",
    "\n",
    "# Assuming `X_list` contains multiple sequences (each sequence is a 2D array of shape (30, n_features))\n",
    "for seq in X_list:\n",
    "    # Transform the sequence using the pre-fitted scaler (no fitting, just transforming)\n",
    "    seq_scaled = scaler.transform(seq)\n",
    "\n",
    "    # Append the scaled sequence to the list\n",
    "    X_real_time_scaled.append(seq_scaled)\n",
    "\n",
    "# Convert the scaled sequences list to a numpy array if needed\n",
    "X_real_time_scaled = np.array(X_real_time_scaled)\n",
    "\n",
    "# Example: Print the shape of the first scaled sequence\n",
    "print(len(X_real_time_scaled[0][1]))  # For checking a single element\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_real_time_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiaise tensors for use in STM training.\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Convert lists of arrays into 3D arrays\n",
    "X_array = np.array(X_real_time_scaled)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_array, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'X_tensor': X_tensor,\n",
    "}, 'D:/GitHub/solar-forecasting/data/data_pipeline.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
