{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Epswf8Gv6yqx"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tSjeesQ6h5x"
   },
   "outputs": [],
   "source": [
    "# Load in data\n",
    "data = torch.load('/content/preprocessed_data.pt')\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_val = data['X_val']\n",
    "y_val = data['y_val']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "sample_weights = data['sample_weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "giDCsD4Q6Xxr",
    "outputId": "2ca4a040-7174-4236-c750-cc68b529c3e4"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Your existing data (already tensors)\n",
    "X = X_train  # full training set (not split yet)\n",
    "y = y_train\n",
    "\n",
    "# Define dataset wrapper\n",
    "class SequenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Model definition\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=1, bidirectional=True, dropout_p=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # last time step\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze()\n",
    "\n",
    "# Grid search parameters\n",
    "param_grid = {\n",
    "    'hidden_size': [32, 64],\n",
    "    'num_layers': [1, 2],\n",
    "    'lr': [0.01, 0.001, 0.0001]\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size = X.shape[2]\n",
    "k_folds = 3\n",
    "best_params = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Grid Search loop\n",
    "for hidden_size in param_grid['hidden_size']:\n",
    "    for num_layers in param_grid['num_layers']:\n",
    "        for lr in param_grid['lr']:\n",
    "            val_losses = []\n",
    "\n",
    "            for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "                model = LSTMModel(input_size, hidden_size, num_layers).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "                criterion = nn.MSELoss()\n",
    "\n",
    "                X_train_fold = X[train_idx]\n",
    "                y_train_fold = y[train_idx]\n",
    "                X_val_fold = X[val_idx]\n",
    "                y_val_fold = y[val_idx]\n",
    "\n",
    "                train_loader = DataLoader(SequenceDataset(X_train_fold, y_train_fold), batch_size=32, shuffle=True)\n",
    "                val_loader = DataLoader(SequenceDataset(X_val_fold, y_val_fold), batch_size=32)\n",
    "\n",
    "                # Training loop for small number of epochs\n",
    "                for epoch in range(50):  # keep small to speed up tuning\n",
    "                    model.train()\n",
    "                    for X_batch, y_batch in train_loader:\n",
    "                        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(X_batch)\n",
    "                        loss = criterion(outputs, y_batch)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Evaluate\n",
    "                model.eval()\n",
    "                fold_val_loss = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for X_batch, y_batch in val_loader:\n",
    "                        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                        outputs = model(X_batch)\n",
    "                        loss = criterion(outputs, y_batch)\n",
    "                        fold_val_loss += loss.item() * X_batch.size(0)\n",
    "                fold_val_loss /= len(val_loader.dataset)\n",
    "                val_losses.append(fold_val_loss)\n",
    "\n",
    "            avg_loss = np.mean(val_losses)\n",
    "            print(f\"Params: hidden={hidden_size}, layers={num_layers}, lr={lr} -> Val Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                best_params = {\n",
    "                    'hidden_size': hidden_size,\n",
    "                    'num_layers': num_layers,\n",
    "                    'lr': lr\n",
    "                }\n",
    "\n",
    "print(f\"\\nBest Params: {best_params} with average CV loss {best_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
