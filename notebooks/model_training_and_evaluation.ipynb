{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C3AUyKTUm8iB",
    "outputId": "25d86e2c-91d3-4de5-8e3a-2944f94e96e4"
   },
   "outputs": [],
   "source": [
    "# Mount google drive for use in google colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kRNYQ7Aas_i"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldkDQgN1auTn"
   },
   "outputs": [],
   "source": [
    "# Load in data\n",
    "data = torch.load('/content/drive/MyDrive/solar_forecasting/preprocessed_data_v2.pt')\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_val = data['X_val']\n",
    "y_val = data['y_val']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "sample_weights = data['sample_weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZkxmlbMcavZu"
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, bidirectional=True, dropout_p=0.3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            dropout=dropout_p, bidirectional=bidirectional,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        # Dropout at fully connected layer\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        # COnvert to fc layer, adjust size depending on bidirectionality.\n",
    "        direction_factor = 2 if bidirectional else 1\n",
    "        self.fc = nn.Linear(hidden_size * direction_factor, 1)  # Regression output\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get last output\n",
    "        out, _ = self.lstm(x)\n",
    "        last_time_step_out = out[:, -1, :]\n",
    "        # Apply MC dropout\n",
    "        dropped = self.dropout(last_time_step_out)\n",
    "        # Output the fully connected layer\n",
    "        out = self.fc(dropped)\n",
    "        return out.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lf9QJr7MToum"
   },
   "outputs": [],
   "source": [
    "# Force MC dropout on by enabling training mode for dropout module during inference.\n",
    "def enable_mc_dropout(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Dropout):\n",
    "            m.train()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRpsZVt5a0nB"
   },
   "outputs": [],
   "source": [
    "# Define weighted MSE loss function to address data imbalance.\n",
    "\n",
    "def weighted_mse_loss(predictions, targets, weights):\n",
    "    return torch.mean(weights * (predictions - targets) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QkQsmoCQa093"
   },
   "outputs": [],
   "source": [
    "# Initialse dataloaders using the train, validate and test datasets respectively.\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SequenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class WeightedSequenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, weights=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.weights = weights if weights is not None else torch.ones(len(X))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.weights[idx]\n",
    "\n",
    "# Use the weighted dataset for training\n",
    "train_dataset = WeightedSequenceDataset(X_train, y_train, sample_weights)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VuWsrtlEa1Pd"
   },
   "outputs": [],
   "source": [
    "# Build test and val loaders\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pp_aXEUla1aP",
    "outputId": "17bc5ca6-d4a8-473b-f437-ce4f9c908e91"
   },
   "outputs": [],
   "source": [
    "# Initialise model and initial parameters from hyperparameter tuning.\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size = X_train.shape[2]  # number of SHARP features\n",
    "model = LSTMModel(input_size=input_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# Scheduler allows tapering of learning rate, by a factor and with a defined patience.\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=300, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jevU6wUta1k8",
    "outputId": "37ef1924-9de0-44ed-ee3a-f53d628bf753"
   },
   "outputs": [],
   "source": [
    "# Train the model, printing loss and validation loss per epoch.\n",
    "\n",
    "num_epochs = 1000\n",
    "best_val_loss = float('inf')\n",
    "patience = 500\n",
    "counter = 0\n",
    "\n",
    "# Loop trhough epochs and batches.\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch, w_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        w_batch = w_batch.to(device)\n",
    "        \n",
    "        # Train model and calculate running loss.\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = weighted_mse_loss(outputs, y_batch, w_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "    \n",
    "    # Calculate loss per epoch.\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    # Get current learning rate.\n",
    "    curr_lr = optimizer.param_groups[0]['lr']\n",
    "    # Print performance to console.\n",
    "    print(f\"[Epoch {epoch+1}] LR: {curr_lr:.6f}\")\n",
    "    # Validation (no weights) - calculate validation loss for each epoch using validation set.\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = nn.functional.mse_loss(outputs, y_batch)\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
    "    # Update scheduler depending on change in validation loss.\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Increment counter to update patience\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0  # Reset counter on improvement\n",
    "        # Save best model using validation performance.\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    \n",
    "    # Use early stopping with patience to prevent overfitting.\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f'Validation loss increased ({val_loss:.4f}), patience {counter}/{patience}')\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "yd9hr1kaa2Lk",
    "outputId": "2784adaa-c67f-498c-c56b-1eb517f8435f"
   },
   "outputs": [],
   "source": [
    "# Use classification to guage model performance, using confusion matrix for performance evauation on test set.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluate model on test set\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "y_pred = np.array(all_preds)\n",
    "y_true = y_test.cpu().numpy()\n",
    "# Convert predictions and targets from log-scale to linear scale\n",
    "y_pred_linear = 10 ** y_pred\n",
    "y_true_linear = 10 ** y_true\n",
    "\n",
    "# Define intensity bins for flare classes\n",
    "bins = [0, 1e-7, 1e-6, 1e-5, 1e-4, np.inf]\n",
    "labels = ['N', 'B', 'C', 'M', 'X']\n",
    "class_indices = list(range(len(labels)))  # [0, 1, 2, 3, 4]\n",
    "\n",
    "# Bin true and predicted values into classes\n",
    "y_true_class = np.digitize(y_true_linear, bins) - 1\n",
    "y_pred_class = np.digitize(y_pred_linear, bins) - 1\n",
    "\n",
    "# Plot confusion matrix\n",
    "unique_labels = sorted(np.unique(np.concatenate([y_true_class, y_pred_class])))\n",
    "display_labels = [labels[i] for i in unique_labels]\n",
    "\n",
    "cm = confusion_matrix(y_true_class, y_pred_class, labels=class_indices)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (Flare Classes from Regression Output)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dljYMkjQa2VM"
   },
   "outputs": [],
   "source": [
    "def mc_predict(model, x_batch, n_passes=50):\n",
    "    model.eval()\n",
    "    enable_mc_dropout(model)\n",
    "\n",
    "    preds = []\n",
    "    for _ in range(n_passes):\n",
    "        with torch.no_grad():\n",
    "            preds.append(model(x_batch).cpu().numpy())\n",
    "\n",
    "    preds = np.stack(preds, axis=0)  # shape: (n_passes, batch_size)\n",
    "    mean = preds.mean(axis=0)        # shape: (batch_size,)\n",
    "    std = preds.std(axis=0)          # shape: (batch_size,)\n",
    "    return mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TPPTjGhXge2"
   },
   "outputs": [],
   "source": [
    "all_means = []\n",
    "all_stds = []\n",
    "\n",
    "model.eval()\n",
    "enable_mc_dropout(model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        batch_mean, batch_std = mc_predict(model, X_batch, n_passes=50)\n",
    "        all_means.extend(batch_mean)\n",
    "        all_stds.extend(batch_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ozxR-kCgXxgC",
    "outputId": "8b04688b-8838-491d-c8b3-d997e46d39b2"
   },
   "outputs": [],
   "source": [
    "all_means = np.array(all_means)\n",
    "all_stds = np.array(all_stds)\n",
    "mean_linear = 10 ** all_means\n",
    "std_linear = (10 ** (all_means + all_stds)) - mean_linear  # approximate upper range\n",
    "\n",
    "for i in range(len(mean_linear)):\n",
    "    print(f\"Sample {i}: Flare = {mean_linear[i]:.2e}, ± {std_linear[i]:.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U-R9FT5UeV48",
    "outputId": "61df8d16-2052-48b9-c7a3-24c38f7d677d"
   },
   "outputs": [],
   "source": [
    "y_true = y_test.cpu().numpy()  # assuming log10-scale targets\n",
    "k = 3  # or 2, depending on how wide you want the tolerance\n",
    "lower_bounds = all_means - k * all_stds\n",
    "upper_bounds = all_means + k * all_stds\n",
    "within_interval = (y_true >= lower_bounds) & (y_true <= upper_bounds)\n",
    "coverage = np.mean(within_interval)\n",
    "print(f\"Coverage within ±{k}σ: {coverage * 100:.2f}%\")\n",
    "# for i, covered in enumerate(within_interval):\n",
    "#     if not covered:\n",
    "#         print(f\"Missed Sample {i}: True = {y_true[i]:.4f}, Pred = {all_means[i]:.4f} ± {k}×{all_stds[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1ygoWJJeeMS",
    "outputId": "34f021d7-16dd-4eed-8de1-c543bc7d475d"
   },
   "outputs": [],
   "source": [
    "y_true = y_test.cpu().numpy()\n",
    "valid_mask = y_true != -9\n",
    "\n",
    "y_true_valid = y_true[valid_mask]\n",
    "means_valid = all_means[valid_mask]\n",
    "stds_valid = all_stds[valid_mask]\n",
    "\n",
    "k = 3  # number of standard deviations for the tolerance\n",
    "\n",
    "lower_bounds = means_valid - k * stds_valid\n",
    "upper_bounds = means_valid + k * stds_valid\n",
    "\n",
    "within_interval = (y_true_valid >= lower_bounds) & (y_true_valid <= upper_bounds)\n",
    "\n",
    "coverage = np.mean(within_interval)\n",
    "print(f\"Coverage within ±{k}σ (excluding -9s): {coverage * 100:.2f}%\")\n",
    "\n",
    "# for i, covered in enumerate(within_interval):\n",
    "#     if not covered:\n",
    "#         print(f\"Missed Sample {i}: True = {y_true_valid[i]:.4f}, Pred = {means_valid[i]:.4f} ± {k}×{stds_valid[i]:.4f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
