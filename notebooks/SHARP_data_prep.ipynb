{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read SHARP data\n",
    "data = pd.read_csv(\"D:/GitHub/solar-forecasting/data/sharp_metadata_dump_daily.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any duplicate writes and inspect data shape and types\n",
    "data = data.drop_duplicates()\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Majority of NOAA_ARS are MISSING, drop column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOAA_ARS contains info about related or overlapping active regions and is often not populated (MISSING).\n",
    "# Too significant to repair, so drop column.\n",
    "noaa_ars_counts = data['NOAA_ARS'].value_counts()\n",
    "\n",
    "print(noaa_ars_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns='NOAA_ARS', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data for nulls.\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all 'MISSING' strings with np.nan\n",
    "data.replace(['MISSING', 'NaN'], np.nan, inplace=True)\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain only QUALITY data. Other data may be corrupted and unsuitable for training.\n",
    "# Corruption can occur through instrument failure or environmental factors such as cosmic rays striking equipment, degrading the image data.\n",
    "data['QUALITY'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain only 'science grade' quality data.\n",
    "quality_data = data[(data['QUALITY'] == 0)]# | (data['QUALITY'] == 65536)]\n",
    "quality_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime object for analysis.\n",
    "quality_data['T_REC'] = pd.to_datetime(quality_data['T_REC'].str.replace('_TAI', ''), format='%Y.%m.%d_%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Null values that do not suit linear interpolation, repair using median value.\n",
    "# Applicable for fields including size and number of pixels, where median is likely to be a reasonable representation.\n",
    "median_features = ['SIZE_ACR', 'SIZE', 'NPIX', 'NACR']\n",
    "\n",
    "for feature in median_features:\n",
    "    medians = quality_data.groupby('HARPNUM')[feature].transform('median')\n",
    "    quality_data[feature] = quality_data[feature].fillna(medians)\n",
    "\n",
    "quality_data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For other features, repair using linear interpolation, which is logical for time dependent magnetic flux measurements.\n",
    "linear_interpolation_features = ['TOTUSJH','TOTUSJZ', 'SAVNCPP', 'USFLUX', 'ABSNJZH', 'TOTPOT', 'MEANPOT', 'MEANJZH', 'SHRGT45', 'MEANSHR', 'MEANJZD', 'MEANALP', 'MEANGBT', 'MEANGBL', 'MEANGAM', 'MEANGBZ', 'MEANGBH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert linear interpolation features to numeric datatypes for analysis and training.\n",
    "quality_data_LI = quality_data.copy()\n",
    "quality_data_LI[linear_interpolation_features] = quality_data[linear_interpolation_features].apply(\n",
    "    pd.to_numeric, errors='coerce'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data, grouping by HARPNUM (solar active region) and sorting by timestamp.\n",
    "df_sorted = quality_data_LI.sort_values(['HARPNUM', 'T_REC']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill nulls using linear interpolation between two adjacent values for all linear interpolation ready features.\n",
    "for col in linear_interpolation_features:\n",
    "    df_sorted[col] = df_sorted.groupby('HARPNUM')[col].transform(lambda g: g.interpolate(method='linear', limit_direction = 'both'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review reduction in null values in set\n",
    "df_sorted.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop small number of remaining null values which could not be repaired using linear interpolation.\n",
    "quality_data_no_null = df_sorted.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm dataset is free of null values and inspect details.\n",
    "quality_data_no_null.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_data_no_null.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_data_no_null.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify number of solar active regions in dataset.\n",
    "len(quality_data_no_null['HARPNUM'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment data into a dictionary containing HARPNUM (patch ID) as key, and list of SHARP sequences as values.\n",
    "harp_dict = {}\n",
    "\n",
    "grouped = quality_data_no_null.groupby('HARPNUM')\n",
    "\n",
    "for harpnum, group in grouped:\n",
    "    harp_dict[harpnum] = group\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(harp_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the length of sequences to build, and the range (+ tolerance) to search through the dataset.\n",
    "sequence_length = 30  # 6 hours of 12-minute cadence\n",
    "cadence_upper = pd.Timedelta(minutes=13)\n",
    "cadence_lower = pd.Timedelta(minutes=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUild a sequence dictionary, with HARPNUM as key and list of valid sequences as value.\n",
    "# Only fully formed consecutive sequences with 12 minute cadence will be retained. Malformed sequences with gaps will be dropped.\n",
    "\n",
    "sequence_dict = {}\n",
    "\n",
    "for harp_ID, sample in harp_dict.items():\n",
    "      \n",
    "    valid_sequences = []\n",
    "    sample = sample.sort_values('T_REC').reset_index(drop=True)\n",
    "\n",
    "    start_idx = 0\n",
    "    while start_idx < (len(sample) - sequence_length + 1):\n",
    "            seq = sample.iloc[start_idx : start_idx + sequence_length]\n",
    "            time_deltas = seq['T_REC'].diff().dropna()\n",
    "\n",
    "            if all(time_deltas < cadence_upper) and all(time_deltas > cadence_lower):\n",
    "                valid_sequences.append(seq.reset_index(drop=True))\n",
    "                start_idx = start_idx + sequence_length\n",
    "            else:\n",
    "                 start_idx += 1\n",
    "    if len(valid_sequences) > 0:\n",
    "        sequence_dict[harp_ID] = valid_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sequence_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find total number of sequences to assess total data quantity.\n",
    "total_sequences = 0\n",
    "lengths = []\n",
    "for sequence_list in sequence_dict.values():\n",
    "    length = len(sequence_list)\n",
    "    lengths.append(length)\n",
    "    total_sequences += length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect sequence dictionary.\n",
    "sequence_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_dict[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign last_timestamp for each sequence to determine the start of the forecast window.\n",
    "flare_sequences = []\n",
    "\n",
    "for harpnum, sequences in sequence_dict.items():\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        last_timestamp = sequence.iloc[29]['T_REC']\n",
    "        NOAA_id = sequence.iloc[29]['NOAA_AR']\n",
    "        flare_sequences.append({\n",
    "            'HARPNUM': harpnum,\n",
    "            'NOAA_id' : NOAA_id,\n",
    "            'Sequence_Number': i,\n",
    "            'Last_Timestamp': last_timestamp\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "flare_sequences_df = pd.DataFrame(flare_sequences)\n",
    "flare_sequences_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find values which are not assigned to a matching NOAA ID (Key required to mergewith  the GOES flare event data)\n",
    "flare_sequences_df['NOAA_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop sequences which are not matched to a valid SHARP patch (HARPNUM == 0). They cannot be validated against flare event data.\n",
    "flare_sequences_df_cleaned = flare_sequences_df[flare_sequences_df['NOAA_id'] != 0]\n",
    "flare_sequences_df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write flare sequences to csv.\n",
    "# Data frame format can be used to match each sequence to a flare event (or no flare) within 24h of the last timestamp.\n",
    "flare_sequences_df_cleaned.to_csv(\"flare_sequences.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GOES flare event data.\n",
    "flare_events = pd.read_csv(\"flare_events.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "flare_sequences_df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "flare_sequences_df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "flare_sequences_df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "flare_sequences_df_cleaned['NOAA_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect flare event data.\n",
    "flare_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert GOES timestamps to datetime objects.\n",
    "flare_events['start_time'] = pd.to_datetime(flare_events['start_time'])\n",
    "flare_events['peak_time'] = pd.to_datetime(flare_events['peak_time'])\n",
    "flare_events['end_time'] = pd.to_datetime(flare_events['end_time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect distribution of flare event classes.\n",
    "flare_events['class_letter'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataframes for merge.\n",
    "seq = flare_sequences_df_cleaned.copy()\n",
    "events = flare_events.copy()\n",
    "\n",
    "# Left join SHARP data sequences to selected fields from GOES flare event data on NOAA_id.\n",
    "merged = seq.merge(events[['noaa_active_region', 'start_time', 'class_letter', 'intensity_W/m^2']], left_on='NOAA_id', right_on='noaa_active_region', how='left')\n",
    "# Creates a cartesian product with many 'duplicate' records. Each sequence per harpnum is matched to every flare event for that harpnum, irresepctive of time.\n",
    "# Data must be filtered to remove invalid dupes.\n",
    "\n",
    "# Drop duplicate key from right table.\n",
    "merged = merged.drop('noaa_active_region', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.shape\n",
    "#Assign 'No flare' as intensity == 10^-9 and class_label == N for all sequences with on flare event within 24h of final timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 24h inclusion mask to extract events with a flare event within 24h.\n",
    "within_24h_mask = (\n",
    "    (merged['start_time'] >= merged['Last_Timestamp']) &\n",
    "    (merged['start_time'] < merged['Last_Timestamp'] + pd.Timedelta(hours=24))\n",
    ")\n",
    "\n",
    "# Set intensity to 0.0 for rows outside the 24h window.\n",
    "merged.loc[~within_24h_mask, 'intensity_W/m^2'] = 0.0\n",
    "merged.loc[~within_24h_mask, 'class_letter'] = \"N\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.drop('start_time', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain sequences with matched flares, and drop their duplicates with no flare event attached.\n",
    "merged = merged.drop_duplicates()\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_flares = merged.loc[merged.groupby(['NOAA_id', 'Sequence_Number'])['intensity_W/m^2'].idxmax()]\n",
    "max_flares = max_flares.sort_index()\n",
    "max_flares.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_flares.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Give 'no flare' events a nominal intensity magnitude of 1e-9\n",
    "max_flares['intensity_W/m^2'] = max_flares['intensity_W/m^2'].replace(0, 1e-9)\n",
    "max_flares.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect distriution of flare intenstiies\n",
    "max_flares['intensity_W/m^2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to log10 scale for intensity.\n",
    "max_flares['log10_intensity'] = np.log10(max_flares['intensity_W/m^2'])\n",
    "max_flares.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove class A. Small number of values barely above background. This intensity will be assigned as 'non-flare'.\n",
    "max_flares = max_flares[max_flares[\"class_letter\"] != \"A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Distribution is highly imbalanced with ~76% of sequences yielding no flare.\n",
    "\n",
    "# Visualise distribution of flare intensities in dataset.\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# sns.kdeplot(max_flares['log10_intensity'], fill=True)\n",
    "# plt.xlabel('log₁₀(Flare Intensity)')\n",
    "# plt.title('Smoothed Distribution of Flare Intensities')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect sequence data.\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', 1000)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sequence_dict[1][1].head()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numeric fields for model training, removing timestamps, AR IDs and quality.\n",
    "\n",
    "sequence_array = a.select_dtypes(include='number')\n",
    "sequence_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data and labels as lists for model training\n",
    "X_list = []\n",
    "y_list = []\n",
    "y_class_label = []\n",
    "\n",
    "# Convert all sequence data into list of numpy arrays, with matching list of intensity labels and also class labels for use in model evaluation.\n",
    "for harpnum, sequences in sequence_dict.items():\n",
    "    for i, sequence_df in enumerate(sequences):\n",
    "        # Get the label\n",
    "        match = max_flares[\n",
    "            (max_flares['HARPNUM'] == harpnum) &\n",
    "            (max_flares['Sequence_Number'] == i)\n",
    "        ]\n",
    "        # if not match.empty:\n",
    "        #     print(f\"Match {harpnum}-{i}\")\n",
    "        if match.empty:\n",
    "        #     print(f\"No match {harpnum}-{i}\")\n",
    "            continue  # Skip if no label\n",
    "\n",
    "        # Select only numeric features \n",
    "        sequence_array = sequence_df.select_dtypes(include='number').to_numpy()\n",
    "\n",
    "        # Should already be shape (30, n_features)\n",
    "        if sequence_array.shape[0] != 30:\n",
    "            print(f\"Sequence != 30 {harpnum}-{i}\")\n",
    "            continue  # optional: skip sequences that don't match expected length\n",
    "\n",
    "        X_list.append(sequence_array)\n",
    "        y_list.append(match['log10_intensity'].values[0])\n",
    "        y_class_label.append(match['class_letter'].values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show distribution of flare classes in dataset.\n",
    "pd.Series(y_class_label).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stratified sampling to split the dataset into 70:15:15 train : validate : test \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split off test set (15%)\n",
    "X_temp, X_test, y_temp, y_test, class_temp, class_test = train_test_split(\n",
    "    X_list, \n",
    "    y_list, \n",
    "    y_class_label, \n",
    "    test_size=0.15, \n",
    "    random_state=42, \n",
    "    stratify=y_class_label\n",
    ")\n",
    "\n",
    "# Split remaining temp set into train and validate sets.\n",
    "X_train, X_val, y_train, y_val, class_train, class_val = train_test_split(\n",
    "    X_temp, \n",
    "    y_temp, \n",
    "    class_temp, \n",
    "    test_size=0.176, \n",
    "    random_state=42, \n",
    "    stratify=class_temp\n",
    ")\n",
    "\n",
    "# Show distribution\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_val))\n",
    "print(len(y_val))\n",
    "print(len(X_test))\n",
    "print(len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce class weigtings to be used with a weighted loss function to address significant class imbalances.\n",
    "# Weights will be inversely proportioal to class frequency.\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Define class list explicitly to maintain consistent order\n",
    "flare_classes = ['N','B', 'C', 'M', 'X']\n",
    "\n",
    "# Count occurrences in training set only\n",
    "class_counts = Counter(class_train)\n",
    "\n",
    "# Total number of training examples\n",
    "total = sum(class_counts.values())\n",
    "\n",
    "# Compute inverse-frequency weights\n",
    "class_weights = {cls: total / class_counts[cls] for cls in flare_classes}\n",
    "\n",
    "# Normalize so sum(weights) = 1\n",
    "norm = sum(class_weights.values())\n",
    "class_weights = {cls: w / norm for cls, w in class_weights.items()}\n",
    "\n",
    "# Display weights \n",
    "for cls in flare_classes:\n",
    "    print(f\"{cls}: weight = {class_weights[cls]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the training data using min-max scaling to prepare for model training.\n",
    "# Apply independently to all sets, train, val and test.\n",
    "# Save scaler for use in live data pipeline, ensuring consistency with training data.\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_train_stacked = np.vstack(X_train) \n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_stacked)\n",
    "\n",
    "# Apply to train set\n",
    "X_train_scaled = [scaler.transform(seq) for seq in X_train]\n",
    "\n",
    "# Sanity check inspection\n",
    "X_train_scaled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply independently to val and test sets\n",
    "\n",
    "X_val_scaled = [scaler.transform(seq) for seq in X_val]\n",
    "\n",
    "X_test_scaled = [scaler.transform(seq) for seq in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiaise tensors for use in LSTM training.\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Convert lists of arrays into 3D arrays\n",
    "X_train_array = np.array(X_train_scaled)\n",
    "X_val_array = np.array(X_val_scaled)\n",
    "X_test_array = np.array(X_test_scaled)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_array, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_array, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_array, dtype=torch.float32)\n",
    "\n",
    "# Targets as float tensors for regression\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data as .pt file for storage and import into model training notebook.\n",
    "\n",
    "sample_weights = torch.tensor([class_weights[cls] for cls in class_train], dtype=torch.float32)\n",
    "torch.save({\n",
    "    'X_train': X_train_tensor,\n",
    "    'y_train': y_train_tensor,\n",
    "    'X_val': X_val_tensor,\n",
    "    'y_val': y_val_tensor,\n",
    "    'X_test': X_test_tensor,\n",
    "    'y_test': y_test_tensor,\n",
    "    'sample_weights': sample_weights,\n",
    "}, 'D:/GitHub/solar-forecasting/data/preprocessed_data_v2.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
